ref:: ![COM4509_6509 Lecture 3.pdf](../assets/COM4509_6509_Lecture_3_1666809397145_0.pdf)

- TODO 47
- 熵(entropy)
	- 熵表示随机变量不确定性的度量
	- 熵的定义
		- 设$X$ 是一个取有限个值的离散随机变量,其概率分布为
		- $P\left(X=x_i\right)=p_i, \quad i=1,2, \cdots, n$
		- 则随机变量 X 的熵定义为
		- id:: a612d7a1-39bf-4a48-abda-9ee38a0ccaa0
		  $$H(S)=-\sum_{i=1}^n p_i \log p_i$$
			- 以2 为底称为 **bit** 或 **shannons**
			- 以 e 为底 称为**nat**(纳特)
			- 以10 为底,称为 **Hartleys**
		- 熵只依赖于$X$的分布,而与 $X$ 的取值无关,所以也可以将$X$ 的熵记作$H(P)$
		- $$H(p)=-\sum_{i=1}^n p_i \log p_i$$
	- 熵越大,变量不确定性越大
		- 从定义可以验证
		- $$0 \leqslant H(p) \leqslant \log n$$
	- 对数的乘法
		- $\log (A \times B)=\log A+\log B$
- 信息增益(information gain)
	- 当我们获得一些有用信息(gain some information)的时候,能够减少熵
	  collapsed:: true
		- 假设$P(Weather=rain)=0.6bits$
		- 可得$H(Weather)=0.97bits$
		- 假设$P(Weather=rain|Sky=cloudy)=0.9 bits$
		- 计算可得$H(Weather |Sky= cloudy )=0.46 bits$
		- 但是我们不会总看到多云,有时候我们会看到晴朗的天空,我们需要所有结果的预测熵
			- $E[H(Weather|Sky)] = p(Sky=cloudy) H(Weather|Sky=cloudy) + p(Sky=clear) H(Weather|Sky=clear)$
		- information gain 是具有和不具有这种观察的熵之间的预期差异
	- 计算公式
		- id:: 634d7ca4-5e42-496b-851c-44c0c1d900ae
		  $$
		  IG(S, A)=H(S)-E[H(S \mid A)] = H(S)-\sum_{i=1}^N p(A=i) H(S \mid A=i)
		  $$
			- $E(x) = p_i * x$
- 例题(熵与信息增益)
	- 抛硬币,正面朝上概率为$80%$,反面为$20%$.
		- 问熵为多少
			- $H(p)=-\sum_{i=1}^n p_i \log p_i = -\left(0.8 \times \log _2(0.8)+0.2 \times \log _2(0.2)\right)=0.72 \text { bits }$
		- 问投掷四次,需要多少位来存储结果,能想一个示例来证明需要少于四位的数据吗?
			- 投掷一次的比特为$0.72$, 则四次为$4\times 0.72=2.88$
			- 假设如果四次全朝上(每次朝上记录为 H,朝下记录为 T),就记录为 1. 否则就记录为 0,同时记录硬币状态,用 1 和 0 分别表示朝上和朝下,一个数字代表一个比特
				- $\mathrm{HHHH} \rightarrow[1]$  [need just one bit]
				- $\mathrm{HTHH} \rightarrow[0,1,0,1,1]$ [need five bits]
				- $\mathrm{THTH} \rightarrow[0,0,1,0,1]$ [need five bits]
			- 抛四次硬币,全为上的概率为$0.8^4=0.41$ 则其余情况的概率为$1-0.41$
			- 所以,预期需要的比特数平均为 $0.41 \times 1+(1-0.41) \times 5=3.36$
		-
	- 垃圾邮件
	  ref::  ((635fd301-4468-48c9-92ab-61d8321074fb))
		- 参考 [L3_DecisionTree.ipynb](http://localhost:8888/notebooks/Lecutre_Note/L3_DecisionTree.ipynb#Lecture3-DecisionTree)
- TODO Information Gain Ratio 信息增益率
- 决策树的生成
	- [[ID3 Algorithm]] ((6362dda6-98fc-463c-9f8d-e835fae1ffdd))
		- ID3算法
			- 给(子)树创建一个根节点
			- if 数据的标签都是相同的，或者属性为空
				- 返回一个具有多数标签的叶子结点
			- 挑选拥有最大 IG 的 attribute A, 以来分割数据
			- 对于attribute A 的每个可能的值 v :
				- 增加一个新的 tree branch, 对应 `A=v`
					- IF 没有数据对应 `A=v`
						- TODO 返回一个具有数据中最常见标签的叶子节点???
					- ELSE
						- 返回 D3(data[A=v], labels[A=v], Attributes – {A})
							- //返回在属性A=v的数据子集上递归调用此方法所计算的子树，并将属性从列表中排除。
			- End
		- ID3 伪码
		  collapsed:: true
			- ```kotlin
			  fun ID3(data,labels,attributes):LeafNode{
			  	val root = RootNode()
			      // labels 全部相同
			      val labelSet = HashSet(labels)
			      if(labelSet.size == 1 ||  arrtibutes.isEmpty()){
			        	return leafNodeWithMajorityLabel
			      }
			      val A = IG.max()
			      for v in A{
			        root.addBranch(v)
			        val eligible = data.filter{ it -> it.attribute == v}
			        if(eligible.size() == 0){
			          return leafNodeWithMostCommonLabel
			        }
			        else{
			          Return ID3(data[A=v], labels[A=v], Attributes – {A})
			        }
			      }
			  }
			  ```
	- Other Choices of Purity
	  collapsed:: true
		- 希望决策树的分支结点所包含的样本尽可能属于同一类别，即**结点的 “纯度” (purity)** 越来越高
	- C4.5算法
- Gini index 基尼系数
	- CART算法 使用 Gini 系数
	- 用每个类别的概率平方之和减去一个类别的概率.
	- id:: 6362e08a-d392-4b8a-b285-8110f7fbe49b
	  $$
	  \operatorname{Gini}(t, \mathcal{D})=1-\sum_{l \in \text { levels }(t)} P(t=l)^2
	  $$
	- 特点
		- 可以这样看待基尼系数,如果你根据数据集中的目标特征概率对实际进行分类,你有多大的频率会错误的分类一个实例.
		- 基尼系数倾向于较大的分区(分布),并且非常容易实现,而信息增益支持较小的分区(分布),有各种不同的值.
	- TODO jupyter 计算 Gini index
- 其他决策树算法
	- 如果你的输出是连续的(你正在做regression)你讲会用到[[Regression Tress]]
		- 我们需要计算一组数据的方差,而不是他的信息增益,以决定去使用哪个信息
		- Regression tree 中,我们一般会使用叶子结点出处的平均值来给叶子分配值
	- 如果你的输入时连续的(continuous)
		- 分割之前,你需要为每个特征找到purity metric(纯度指标)最大化的阈值
- 决策树的修剪 Pruning
  collapsed:: true
	- 决策树中的过拟合overfitting问题
		- 过度拟合发生的可能性,随着树的深入而增加,因为所产生的分类是基于越来越小的子集,因为数据集在路径中的每个特征测试后都被分割.
	- 预修剪 Pre-pruning ((6362e664-c631-4ab1-8800-046b98faaff7))
		- 如果错误的减少不足以证明增加一个额外的子树的额外复杂性,就停止继续生成树
	- 后修剪 Post-pruning
		- 允许算法随心所欲地生长树,然后修剪导致过度拟合的树枝
- TODO 特征选择
	- 特征选择在于选取对训练数据具有分类能力的特征,以提高决策树的学习效率
	- 互信息 mutual information
		- 熵 $H(Y)$ 与条件熵 $H(Y \mid X)$之差
	- 根据信息增益准则的特征选择方法
		- 对训练数据集D,计算其每个特征的信息增益,并且比较他们的大小,选择信息增益最大的特征
- TODO [[随机森林]]
- Ensembles 集成学习
	- 集成学习的核心思路就是“人多力量大”，它并没有创造出新的算法，而是把已有的算法进行结合，从而得到更好的效果
	- 组装这些模型的思路主要有两种
		- boosting
		- bagging
	- Boosting 算法
		- 核心思路
			- 挑选精英,与bagging最本质的差别在于他对基础模型不是一致对待的，而是经过不停的考验和筛选来挑选出“精英”，然后给精英更多的投票权，表现不好的基础模型则给较少的投票权，然后综合所有人的投票得到最终结果
		- 大部分情况下，**经过 boosting 得到的结果偏差（bias）更小**
		- 具体过程
			- 通过加法模型将基础模型进行线性的组合。
			- 每一轮训练都提升那些错误率小的基础模型权重，同时减小错误率高的模型权重。
			- 在每一轮改变训练数据的权值或概率分布，通过提高那些在前一轮被弱分类器分错样例的权值，减小前一轮分对样例的权值，来使得分类器对误分的数据有较好的效果
	- [[AdaBoost]]
		- 是boosting 算法的其中一种
		- 优缺点
			- 很好的利用了弱分类器进行级联
			- 可将不同的分类算法作为弱分类器
			- AdaBoost 具有很高的精度
			- 相对于bagging算法和[[Random Forest]]AdaBoost 充分考虑每个分类器的权重
			- ---
			- AdaBoost迭代次数也就是弱分类器数目不太好设定，可以使用交叉验证来进行确定；
			- 数据不平衡导致分类精度下降；
			- 训练比较耗时，每次重新选择当前分类器最好切分点；
	- Bootstrapping
	- Bootstrap Aggregating (Bagging)
	- Bagging for Random Forest