file-path:: ../assets/COM4509_6509_Lecture_1_1664108573059_0.pdf
file:: [COM4509_6509_Lecture_1_1664108573059_0.pdf](../assets/COM4509_6509_Lecture_1_1664108573059_0.pdf)
title:: hls__COM4509_6509_Lecture_1_1664108573059_0

- Model
  ls-type:: annotation
  hl-page:: 32
  id:: 63305086-7b11-4a11-8c54-25d84c632670
- Data
  ls-type:: annotation
  hl-page:: 32
  id:: 6330508f-7708-4987-9e20-d5cd50890016
- leave-one-out cross validation
  ls-type:: annotation
  hl-page:: 37
  id:: 633050fc-b046-4716-942e-a36f0cf60685
- on the training set?
  ls-type:: annotation
  hl-page:: 47
  id:: 6330621f-efda-4189-be6d-e58e11a2d49c
- on the held-out cross validation data?
  ls-type:: annotation
  hl-page:: 47
  id:: 63306230-b0df-4655-aa4e-1857e6ab1a85
- What happens at higher orders
  ls-type:: annotation
  hl-page:: 47
  id:: 6330625d-2aaa-4e3a-b486-4e51a089889b
- A model’s ability to predict other data (especially from an unseen dataset) is called generalisation.
  ls-type:: annotation
  hl-page:: 49
  id:: 633062b4-b5d2-4add-a7f5-290e34fae4d5
- Similar groups [clustering]
  ls-type:: annotation
  hl-page:: 55
  id:: 633067b0-8d2f-4356-b267-c80ecfda0ee8
- A probability density function [density estimation]
  ls-type:: annotation
  hl-page:: 55
  id:: 633067b6-5d00-4e0f-9cb6-2e8b3aa6fc83
- A better representation [e.g. dimensionality reduction]
  ls-type:: annotation
  hl-page:: 55
  id:: 633067bb-0cd3-43b9-9b86-1fe58048297f
- ordinary least squares’
  ls-type:: annotation
  hl-page:: 56
  id:: 6330685e-8133-4b06-89a2-23cefcab3671
- Problems with Machine Learning
  ls-type:: annotation
  hl-page:: 57
  id:: 633068bf-7fa2-4991-9221-854434e7bddc
- Objective Function
  ls-type:: annotation
  hl-page:: 56
  id:: 633068c8-f4f1-4d70-a72b-7c8b0fed7e66
- - Generalisation is the ability for an algorithm to make good predictions on other similar datasets.
  ls-type:: annotation
  hl-page:: 58
  id:: 63306a59-592c-4a4d-b407-e1ec72f8b265
- - You might want to select between models/hyperparameters: 
  ls-type:: annotation
  hl-page:: 58
  id:: 63306a62-6334-4e6d-9918-7cd63bb0669a
- Random Variable (RV)
  ls-type:: annotation
  hl-page:: 67
  id:: 6330858c-f662-4c58-9d82-8e749b7a3e09
- P(X) is the probability mass function
  ls-type:: annotation
  hl-page:: 70
  id:: 63308607-3052-49f1-a98e-9b52379f1122
- probability mass function.
  ls-type:: annotation
  hl-page:: 71
  id:: 63308626-56aa-4a84-aaa2-c59544ed6b57
- In Bayesian reasoning this is known as the prior. It’s what we believe prior to any observations.
  ls-type:: annotation
  hl-page:: 72
  id:: 633089da-f59e-4033-a4b9-7bb2c09a76b2
- Note that this is different from the JOINT probability:
  ls-type:: annotation
  hl-page:: 75
  id:: 63308a54-5833-4197-ac32-54c103ece8e0
- product rule of probability
  ls-type:: annotation
  hl-page:: 76
  id:: 6330942b-3070-45c8-8cbf-aa1d7d0666ae
- what is the probability that the escaped animal is a tiger and from enclosure B
  ls-type:: annotation
  hl-page:: 85
  id:: 6331fa58-1e33-47c3-b517-3cd6aa515bf7
- Bayes’ Theorem
  ls-type:: annotation
  hl-page:: 86
  id:: 6331fe10-2018-4d6a-a9fd-35ae61a8c161
- Marginalisation
  ls-type:: annotation
  hl-page:: 91
  id:: 63320ced-d4ff-4bed-80da-1b3cfac041f0
- we can add these together to get the probability of having jam on his jumper:
  ls-type:: annotation
  hl-page:: 91
  id:: 63320f16-390d-482c-be7d-3404d9a1ef3d
- We next need P(J|¬W).
  ls-type:: annotation
  hl-page:: 92
  id:: 63320f1e-2bfc-4248-beed-b99888b48bc8
- Who stole the scone
  ls-type:: annotation
  hl-page:: 62
  id:: 63321251-cf29-4c82-8e9c-dc0cc39894eb
- Biggest confusion: Joint vs Conditional
  ls-type:: annotation
  hl-page:: 73
  id:: 63321704-9ebd-43f6-a04f-3fff61109aff
- regression
  ls-type:: annotation
  hl-page:: 6
  id:: 6332bd6b-261f-484f-95d2-608e4233d999
- statistical learning
  ls-type:: annotation
  hl-page:: 6
  id:: 6332bd8e-7581-463c-90e5-f6198482bdf4
- predictions
  ls-type:: annotation
  hl-page:: 6
  id:: 6332bda7-5b22-4edf-aa8d-5f233c36391f
- Let’s get started
  ls-type:: annotation
  hl-page:: 20
  id:: 6332bfd1-9b87-4c1e-9dc3-3a3466fe837b
- Definitions
  ls-type:: annotation
  hl-page:: 31
  id:: 6332c176-8aea-44ff-87b6-3c020fc8554d
- Linear regression
  ls-type:: annotation
  hl-page:: 4
  id:: 6332c815-8f51-412d-ae16-d1dcf03980ef
- Decision trees and ensemble methods
  ls-type:: annotation
  hl-page:: 4
  id:: 6332c9a6-9e21-4b35-bb02-9a6ec02c921d
- [:span]
  ls-type:: annotation
  hl-page:: 91
  id:: 6332d1c7-1436-4054-a2b8-9591f3ddcf80
  hl-type:: area
  hl-stamp:: 1664274886739
- Derivation of least squares linear regression
  ls-type:: annotation
  hl-page:: 107
  id:: 6332d631-f987-4389-b571-6803e92ecfd8
- Take Home Messages
  ls-type:: annotation
  hl-page:: 106
  id:: 6332d63b-4f47-4be3-9c6c-3fc3d74e2327
- Estimating moments
  ls-type:: annotation
  hl-page:: 105
  id:: 6332d640-f88f-4b74-a930-aeaeba31ec58
- Conditionally independent variables are independent when a third variable is fixed.
  ls-type:: annotation
  hl-page:: 109
  id:: 633bfa0f-12cf-4a98-b087-7bd371e9d22a
- If two variables are independent then the joint probability (or joint probability density) of the two of them is equal to the product of the two probabilities (or probability densities)
  ls-type:: annotation
  hl-page:: 109
  id:: 633bfa15-2a1c-4d4c-9a08-611e1f28a834
- Independence and Conditional Independence
  ls-type:: annotation
  hl-page:: 109
  id:: 633bfa1e-3450-404e-b8d1-0d4952818639